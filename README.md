# Updating, Maintenance & Scaling
So far, our team has done the basic setup of our object detection pipeline for security systems. If you followed all the steps covered in previous six writeups, you should have got a similar pipeline that can automatically run on a remote server to validate images, detect objects as well as threats, send alerts and then generate a plot. However, we haven't reach the end of the project. This writeup discusses what you can do in the long run to update, debug, maintain or scale your pipeline, followed by some ideas of improvement you may apply to your pipeline.
## Updating the Pipeline
After the initial deployment, it's common you may want to update the pipeline. 

To update the specification of a certain stage, you just need to update the associated JSON file and execute "update-pipeline" with pachctl. It's the same way you set the API key to the threat detect stage as described in writeup 6.
```
$ pachctl update-pipeline -f <pipelineName.json>
```
Things get a little complicated if you want to update your code. No matter whether you are adding a new stage to the pipeline or modifying codes of existing stages, you should never directly modify your codes running in the pipeline. Instead, try to develop a better algorithm and fully test it locally before deploying it elsewhere.  Since the container running in the pipeline is the old container with the old codes, to replace what’s in the pipelines with what you have locally, you need to rebuild your docker images. If you add some dependencies in your code, you might have to change the original dockerfile accordingly. In other cases, you can run the same dockerfile to generate a new version of docker images, so that the new code goes in there rather than the old code. The next step is to push up that new docker image to Docker Hub. Then as mentioned above, update the associated JSON file and execute "update-pipeline" to tell the cluster to stop the old container and run the new container with new codes. Make sure you get the new docker images each a unique tag, which corresponds to the new version of your code. Otherwise, Pachyderm will think the specific images are already pulled and running, so it won't stop the pipeline and pull the new version as you expect. There is no restriction on how you can tag your image, and it can be as simple as a version number. But it's better to avoid vague tag names that are hard to keep track of, such as "my fifth version from last Wednesday". Also, you can automatically generate your tag with your GitHub commit. Every time you make a commit to GitHub, it automatically builds the docket image and tags that with a commit id, you can always tag exactly that version to your code.

If you have concerns about the potential lag in updating process or the functionality of the updated pipeline, you can create a separate pipeline that subscribes to the same data, and run it in parallel with the other to make a comparison between the two versions. If the new version turns out to be satisfying, you can just update to the new version, and delete the old one.
## Debugging and Maintenance
Although you can ensure the pipeline is well-designed and fully-tested, things can still go wrong. In that case, you need some approaches to debug, so that you know which part of the code goes wrong. Also, in routine maintenance, as you update your pipeline from time to time, you may for some reason want to know the version of code related to a specific result, which requires proper documentation as well.

In addition to your own logs of your codes, data provenance in Pachyderm offers detailed tracking from inputs to outputs. If you run things multiple times, every change you made to any collection of data is versioned and tracked by Pachyderm, and so are information about what program has processed what data at what time. Each output has a job ID associated with it. Just as you can see locally which program runs on what input and gives what output, you can get the same information when the program running somewhere else. When in Docker, you can access your logs directly via Docker, but via Pachyderm would be more convenient. When you do "get pods" with "kubectl", you will get all pods of the pipelines running. Then you can execute "logs" for a certain pod/container, which will show all the jobs in that container, and each job should have a job id attached in the name. With a similar “get logs” command in “pachctl” using the target job ID, you can get the IDs for preceding steps and get the log for those jobs to see if there’s any error in the codes. What’s more, since you know the inputs and outputs of all steps leading to a particular result, you can actually reproduce any results.
```
$kubectl logs [-f] [-p] <pod> [-c <container>]
```
```
$pachctl get-logs [--pipeline=<pipeline>|--job=<job id>]
```
## Distributed Computing
Having everything running in one node is fine within a small scale. However, if you want to scale up and your pipeline starts to process thousands of images per minute, then probably the resource of one node is not enough. Especially, stages like the object detection in our pipeline would become a bottleneck, taking a lot of time. To solve the problem, you can spin up multiple instances and distribute the work of one stage across those instances. No change in code is needed. You just need to update the pipeline as illustrated above, letting Pachyderm and k8s know that you want to share data and run a specific stage across a certain number of instances. Then Pachyderm will help you spin up the instances needed and when images coming in, automatically split those files across workers to process in parallel. Also, you can turn on autoscaling, for which k8s will spin up more workers for you whenever necessary.
## Improvements, Next Steps
The pipeline described in this series of writeups is built towards the minimum level of functionality required. Plenty of room exists for improvement. Following are two possible steps worth a try.

First, you can modify your pipeline to include video feed from cameras. Current model only considers the inputs from motion triggered camera. If you expect most of your inputs to be videos, you can add an initial stage to convert the format from video to a collection of images, and then group them under a directory. 

Second, instead of processing per image, you may process images by batch, in terms of corresponding to a particular event, especially if your inputs are videos. At least for notification stage, rather than sending out an email each time a threat is detected, it makes more sense to send one email per threat event.
