# Updating, Maintenance & Scaling
So far, our team have done the basic setup of our object detection pipeline for security systems. If you followed all the steps covered in previous six writeups, you should have got a similar pipeline that can automatically run on a remote server to validate images, detect objects as well as threats, send alerts and then generate a plot. However, we haven't reach the end of the project. This writeup discusses what you can do in the long run to update, debug, maintain or scale your pipeline, followed by some ideas of improvement you may apply to your pipeline.
## Updating the Pipeline
After the initial deployment, it's common you may want to update the pipeline. 

To update the specification of a certain stage, you just need to update the associated JSON file and execute "update-pipeline" with pachctl. It's the same way you set the API key to the threat detect stage as described in writeup 6.
```
$ pachctl update-pipeline -f <pipelineName.json>
```
Things get a little complicated if you want to update your code. No matter whether you are adding a new stage to the pipeline or modifying codes of existing stages, you should never directly modify your codes running in the pipeline. Instead, try to develop a better algorithm and fully test it locally before deploying it elsewhere.  Since the container running in the pipeline is the old container with the old codes, to replace what’s in the pipelines with what you have locally, you need to rebuild your docker images. If you add some dependencies in your code, you might have to change the original dockerfile accordingly. In other cases, you can run the same dockerfile to generate a new version of docker images, so that the new code goes in there rather than the old code. The next step is to push up that new docker image to Docker Hub. Then as mentioned above, update the associated JSON file and execute "update-pipeline" to tell the cluster to stop the old container and run the new container with new codes. Make sure you get the new docker images each a unique tag, which corresponds to the new version of your code. Otherwise, Pachyderm will think the specific images are already pulled and running, so it won't stop the pipeline and pull the new version as you expect. There is no restriction on how you can tag your image, and it can be as simple as a version number. But it's better to avoid vague tag names that are hard to keep track of, such as "my fifth version from last Wednesday". Also, you can automatically generate your tag with your GitHub commit. Every time you make a commit to GitHub, it automatically builds the docket image and tags that with a commit id, you can always tag exactly that version to your code.

If you have concerns about the potential lag in updating process or the functionality of the updated pipeline, you can create a separate pipeline that subscribes to the same data, and run it in parallel with the other to make a comparison between the two versions. If the new version turns out to be satisfying, you can just update to the new version, and delete the old one.
## Debugging and Maintenance
Although you can ensure the pipieline is well-desinged and fully-tested, things can still go wrong. In that case, you need some approaches to debug, so that you know which part of the code goes wrong. Also, in routine maintenance, as you update your pipieline from time to time, you may for some reason want to know the version of code related to a specific result, which requires proper documentation as well. 

 so all of that you can get form a combination of your logs to your code and the proviance traking in paetyderm.
Just as you can see locally which programs runs and gives what output, you can get the same information when the program running somerhere esle. When in Docker, you can still acess your logs directly via Docker, but via Pachyderm would be more convienet beacsue it 
 If you running a poyhton program, get you ar putting an ouput , you can see that when you loaclaly, if you run it osmehtinger else, 
 
When you do "get pods" with "kubectl", you will get all pods of the pipelines running. Then you can excecute "get logs" for a ceratin pod/contianer, which will show all the jobs in that pod/contianer, and each job should have a job id attached in the name. You can get the With the same command, you can get the log for  ican grabe the nmaen one of this pods nad o cude control logs po menasi pod, its going to I have abunkvs of jobs, for each of this is a java id, garbe that java id and get the log for that java with the command, you souhl eb following th same way you would be loging in in a normal python program, you cen log and lokk at this thing to see is there any ereor.
 
If you run things multiple times, in your pipilien, you are versioning each of this colecitons of data, everytihg change we made to on eoftheocllecitons of data is versioned, and also I know what has proceosed wht data t what time, that’s the imagemation fo the java, spot he sombinations of the two of this things gives us idea whtere is probpke. If im traking hat imfroamtion, 
 I can lok at for exxapmle an output like this plot, and icna say what were all the other pieces awhat was the stage of the other piece of data when that plot wa generated so your data and code is chaning all the time, you want to know anypoint at time, what wre all the piece of data nd code that produce this specific results at the specifc time. 
 This is another jobrun pricoiesly in history, I wna tot knowwhteres all piece sof dat being generated. Java id, ad then there is this bit of imafoee, which is the commit id of the output, with git, when you make a commit, there si id with change. So in the same way, the version collection of data, whanever, thet is an othout pot input o rhcnage to naypievce of data, ifd asocoiated, theis is the id ascoaiiiied with job for tha tproericlaur plot, ispect that comit of plot and what well see this is ishtis nice reperosetation,this is the stage of validaita of my vaidlation dta when plot wad generated, this is the dtat from object dection output,..eta. and put ervertyihign back together, so you can actually reproceruse any particular resuls. Fro any stage anty piece of data theries I coid id, lets get therules file at this partiam time, get a this me this pratiglar file in genterating the plot, go backto the prievous verison. It will give me back whaereve. Here is th pipilien, here is th eimformation about input was, you ere usin gwhta stage. Allthe stausts of data and docker image. I can know hat happned and dlei with them. Get file rules master, list file theret detect, I can seethere is one directory. 
```
$kubectl logs [-f] [-p] <pod> [-c <container>]
```
```
$pachctl get-logs [--pipeline=<pipeline>|--job=<job id>]
```
## Distributed Computing
Having everything running in one nodes is fine within a small scale. However, if you want to scale up and your pipeline starts to process thousands of images per minute, then probably the resource of one node is not enough. Especially, stages like the object detection in our pipeline would become a bottleneck, taking a lot of time. To solve the problem, you can spin up multiple instances and distribute the work of one stage across those instances. No change in code is needed. You just need to update the pipeline as illustrated above, letting Pachyderm and k8s know that you want to share data and run a specific stage across a certain number of instances. Then Pachyderm will help you spin up the instances needed and when images coming in, automatically split those files across workers to process in parallel. Also, you can turn on autoscaling, for which k8s will spin up more works for you whenever necessary.
## Improvements, Next Steps
The pipeline described in this series of writeups is built towards the minimum level of functionality required. Plenty of room exists for improvement. Following are two possible steps worth a try.

First, you can modify your pipeline to include video feed from cameras. Current model only considers the inputs from motion triggered camera. If you expect most of your inputs to be videos, you can add an initial stage to convert the format from video to a collection of images, and then group them under a directory. 

Second, instead of processing per image, you may process images by batch, in terms of corresponding to a particular event, especially if your inputs are videos. At least for notification stage, rather than sending out an email each time a threat is detected, it makes more sense to send one email per threat event.
